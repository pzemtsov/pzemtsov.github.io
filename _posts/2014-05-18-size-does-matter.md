---
layout: post
title:  Size does matter
date:   2014-05-18 12:00:00
tags: C C++ GCC optimisation
ART-CFAST: /2014/05/15/how-to-make-C-as-fast-as-java.html
TITLE-CFAST: "How to make C run as fast as Java"
ART-UNSTABLE: /2014/05/12/mystery-of-unstable-performance.html
TITLE-UNSTABLE: "The mystery of an unstable performance"
---

I finished [my previous article]({{ page.ART-CFAST }}) with the observation that, despite the optimisation effort,
the code generated by the GNU C compiler was still not optimal.

What's wrong with the code?
---------------------------

Let's concentrate on the `Dst_First_3a`.

[Here is its **C++** source]({{ site.REPO-E1-C }}/blob/c976a9f9d44345859ac6b4c4b81dc20527842575/e1.cpp):

{% highlight C++ %}
class Dst_First_3a : public Demux
{
public:
    void demux (const byte * src, unsigned src_length, byte ** dst) const
    {
        assert (src_length == NUM_TIMESLOTS * DST_SIZE);

        for (unsigned dst_num = 0; dst_num < NUM_TIMESLOTS; ++ dst_num) {
            byte * d = dst [dst_num];
            for (unsigned dst_pos = 0; dst_pos < DST_SIZE; ++ dst_pos) {
                d [dst_pos] = src [dst_pos * NUM_TIMESLOTS + dst_num];
            }
        }
    }
};
{% endhighlight %}

We compiled it with function alignment and loop alignment set to 32 and loop unrolling switched on.
[This is the assembly output](({{ site.REPO-E1-C }}/blob/c976a9f9d44345859ac6b4c4b81dc20527842575/e1.asm):

{% highlight text %}
_ZNK12Dst_First_3a5demuxEPKhjPPh:
        subq    $8, %rsp
        cmpl    $2048, %edx
        jne     .L40
        xorl    %r9d, %r9d
        .p2align 5
.L28:
        movq    (%rcx,%r9,8), %rdi
        movl    %r9d, %edx
        xorl    %eax, %eax
        .p2align 5
.L29:
        movl    %edx, %r8d
        leal    32(%rdx), %r10d
        movzbl  (%rsi,%r8), %r11d
        movb    %r11b, (%rdi,%rax)
        movzbl  (%rsi,%r10), %r8d
        leal    64(%rdx), %r11d
        movb    %r8b, 1(%rdi,%rax)
        movzbl  (%rsi,%r11), %r10d
        leal    96(%rdx), %r8d
        movb    %r10b, 2(%rdi,%rax)
        movzbl  (%rsi,%r8), %r11d
        leal    128(%rdx), %r10d
        movb    %r11b, 3(%rdi,%rax)
        movzbl  (%rsi,%r10), %r8d
        leal    160(%rdx), %r11d
        movb    %r8b, 4(%rdi,%rax)
        movzbl  (%rsi,%r11), %r10d
        leal    192(%rdx), %r8d
        movb    %r10b, 5(%rdi,%rax)
        movzbl  (%rsi,%r8), %r11d
        leal    224(%rdx), %r10d
        addl    $256, %edx
        movb    %r11b, 6(%rdi,%rax)
        movzbl  (%rsi,%r10), %r8d
        movb    %r8b, 7(%rdi,%rax)
        addq    $8, %rax
        cmpq    $64, %rax
        jne     .L29
        addq    $1, %r9
        cmpq    $32, %r9
        jne     .L28
        addq    $8, %rsp
        .cfi_remember_state
        .cfi_def_cfa_offset 8
        ret
{% endhighlight %}

Let's look at the inner loop (the code between `.L29` and `jne .L29`. This loop has been unrolled 8 times,
this is why the same piece of code is repeated 8 times:

{% highlight text %}
        leal    64(%rdx), %r11d
        movb    %r8b, 1(%rdi,%rax)
        movzbl  (%rsi,%r11), %r10d
{% endhighlight %}

(the offsets in `leal` and `movb` vary).

Here `%rdx` is a pointer to the source memory, where the input bytes reside at the offsets 0, 32, 64 and so on;
these bytes are loaded and written to the memory at the offsets 0, 1, 2, etc, from `(%rdi + %rax)`.
The `%rdi` is `d` and `%rax` is `dst_pos`.

Why is there the `leal` instruction? All it does is that it adds 64 to `%rdx` and writes result into `%r11d` (the
`%r11d` is the lowest 32 bits of the 64-bit register `%r11`; when something is written to these lower 32
bits, the higher 32 bits are zeroed). The result of this addition is later used in `movzbl`, where another
register is added to it - `%rsi` (`src`).  But wait, in Intel architecture an addressing mode can consist of
two registers (one possibly scaled) and an offset; why is this `64` offset not placed right there, like this?

{% highlight text %}
        movb    %r8b, 1(%rdi,%rax)
        movzbl  64(%rsi,%rdx), %r10d
{% endhighlight %}

The hint comes from the name of the instruction (`leal`, rather than `leaq`), as well as the name of the result
(`%r11d`): this is a 32-bit instruction. Why is the compiler computing the sum in 32 bits, but uses the 64-bit
version of the result in the addressing mode?

The reason is simple: `dst_pos` is declared as `unsigned`, and `unsigned`, as well as `int`, is 32 bits in GCC.
The `leal` here has nothing to do with "load effective address", which is what its name stands for; it is used as an
addition instruction, and the only reason it is used instead of `addl` is that it allows the result to be in
different register from the source, which saves a move instruction in our case.

In the source code `dst_pos` is multiplied by `NUM_TIMESLOTS`, and this is a 32-bit multiplication. Even if the
result is converted to a 64-bit value later, the **C++** requires the multiplication to be performed in 32-bit
mode, and the high part of a result to be discarded. Check this:

{% highlight C++ %}
unsigned x = 1000000;
unsigned long y = x * x;                 // result is 3567587328, which is
                                         // 1000000000000L & 0xFFFFFFFFL
unsigned long z = (unsigned long) x * x; // result is 1000000000000
{% endhighlight %}

In our example multiplication was replaced by addition (as a result of the strength reduction), but that addition was still performed
in 32 bits for numeric safety. If current value of `%edx` is close to `0xFFFFFFFF`, adding 64 to it can cause
overflow and produce some small number as a result; this result would in fact be correct, while 64-bit result
(where no overflow occurs, and the result is big) would be incorrect. This means we can't embed this addition into the addressing mode
of a load instructions, because address calculations are always 64 bits on this processor.

We know that in this loop `dst_pos` is actually small, it stays within interval `[0..DST_SIZE-1]`. If the compiler
established this fact (and there are techniques of an interval analysis that make it possible), it could use the
most efficient operand size instead of the declared size. The results would not differ because no overflow would
ever happen. Unfortunately, it seems that the compiler does not perform this optimisation.

Why are `int` and `unsigned` 32 bits?
-------------------------------------

**C/C++** is quite relaxed about data type sizes. It requires `int` to be the same size as `unsigned`, both to
be at least 16 bits, both to be not smaller than `short` and not longer than `long`. I still remember compilers
where `int` was 16 bits, it was like that on all 8- and 16-bit platforms. 32-bit platforms established a new
standard, and for a long time the most common size of `int` has been 32 bits. The standard does not prevent
changing it to 64 bits.

I can think of two reasons why this hasn't happened in GNU C. One is the legacy code. There is a huge amount of **C** code
around, it is everywhere. Not all of it is well-written. Some of the code may contain dependencies on type
sizes, and it is very difficult to find these dependencied in the code.

The other reason is that 64-bit isn't always better. Unofficially **C/C++** declares that `int` is the most natural
and the most efficient integer data format on the target architecture. We've seen that for the purpose of array indexing
the 64-bit type is indeed the most efficient. However, there are other cases.

First of all, the instruction encoding is implemented in such a way that the shortest instructions are 32-bit instructions
working with traditional registers (`eax`, `edx`, etc.):

     01 C3         addl %eax, %ebx
     48 01 C3      addq %rax, %rbx

this means that the 32-bit code is shorter. However, this is only applicable to small functions that can be fully
compiled using only traditional registers. As soon as the new registers (`r8` .. `r15`) are used, the instructions
immediately become as long as in the 64-bit case:

     49 01 C2        addq %rax, %r10
     41 01 C2        addl %eax, %r10d

This means that the negative effect of working with 64-bit registers is relatively small for real programs.

There are 64-bit instructions that execute much longer than their 32-bit versions. Division is one example.
Multiplication seems to run at the same speed.

64-bit numbers take twice the amount of space in data structures. It causes programs to use more memory and can
reduce speed due to memory bandwith limitations.

32-bit numbers often allow better low-level optimisations than 64-bit numbers. Here is a simple example --
a routine that adds two vectors of of 256 numbers each. We'll make two versions: for 32-bit numbers and for 64-bit:

{% highlight C %}

void sum32 (unsigned *a, unsigned * b, unsigned * c)
{
    unsigned i;
    for (i = 0; i < 256; i++) a[i] = b[i] + c[i];
}

void sum64 (unsigned long *a, unsigned long *b, unsigned long *c)
{
    unsigned long i;
    for (i = 0; i < 256; i++) a[i] = b[i] + c[i];
}

{% endhighlight %}

I won't show the entire code here, it is surprisingly big
([you can see it in the repository]({{ site.REPO-E1-C }}/commit/4ae5a53f64aa99b946f4d26b65122a8ed64bf66e)).
I'll only show the main loops. Here is one for `sum32`:

{% highlight text %}
.L5:
        addl    $1, %esi
        paddd   (%r8), %xmm0
        addq    $16, %r8
        cmpl    %r9d, %esi
        jb      .L5
{% endhighlight %}

And here is one for `sum64`:

{% highlight text %}
.L17:
        addq    $1, %rsi
        paddq   (%r8), %xmm0
        addq    $16, %r8
        cmpq    %r9, %rsi
        jb      .L17
{% endhighlight %}

The fragments look almost identical, except for one important difference: `paddd` instruction in the first case
and `paddq` in the second. These are the [**SSE-2**](http://en.wikipedia.org/wiki/SSE2) instructions.
Both operate on 128-bit `xmm` registers, but `paddd`
considers these registers consisting of four 32-bit words, while `paddq` of two 64-bit words.
The operations are performed simultaneously on all the components.

The code is really amazing. The GNU C compiler has a built-in vectoriser, which tries to use SSE wherever possible.
In this case it has split a source vector into sub-vectors of sizes four or two, added those sub-vectors component-wise,
and then calculated the sum of the result.

Obviously, the second code requires twice as many iterations, as a result it runs longer.

As you can see, there isn't a simple answer to the question which integer type is the most natural on the Intel 64
architecture, and there are valid arguments both ways. However, it seems that 64 bits is the best size for values
used to index arrays.

Going 64 bits
-------------

Let's take `Dst_First_3a` and declare `dst_pos` a 64-bit variable. On 64-bit GCC we can use `unsigned long`:

{% highlight C++ %}
class Dst_First_3a : public Demux
{
public:
    void demux (const byte * src, unsigned src_length, byte ** dst) const
    {
        assert (src_length == NUM_TIMESLOTS * DST_SIZE);

        for (unsigned dst_num = 0; dst_num < NUM_TIMESLOTS; ++ dst_num) {
            byte * d = dst [dst_num];
            for (unsigned long dst_pos = 0; dst_pos < DST_SIZE; ++ dst_pos) {
                d [dst_pos] = src [dst_pos * NUM_TIMESLOTS + dst_num];
            }
        }
    }
};
{% endhighlight %}

When we run it, we get the following:

    12Dst_First_3a: 652

And the previous result we got (as published in ["{{ page.TITLE-CFAST }}"]({{ page.ART-CFAST }}))

    12Dst_First_3a: 727

We've got very significant improvement. It looks attractive to modify all the code in the same way.

Going portable
--------------

Replacing `unsigned` with `unsigned long` throughout the program seems a bit unsafe. What if it is bad for some
other platform? If some 32-bit platform defines `unsigned long` as 64-bit, such a modification will cause
a disaster. What we need is a special type for variables used as indices. Fortunately for us, such a type
exists, it is the `size_t`.

Strictly speaking, the `size_t` isn't designed for indices, it is there for sizes. A `sizeof` operator returns
results of this type. Many library functions have parameters of this type: `malloc()`, `calloc()`,
`memcpy()`, `qsort()` and others. In addition to byte counts, it is used for element counts. It looks
very reasonable to use this type for indexing, as long as a programmer is happy to use an unsigned type. There
is a signed version of the `size_t`, too -- it is called `ptrdiff_t`.

Our index variables are unsigned already, so we can make all of them `size_t`.
It makes sense to modify our interface and make `src_length` of type `size_t` as well. Besides making the code
consistent (we won't compare `size_t` variables with `unsigned`), this has another positive effect: all the
versions that do not hard-code the input size will be able to work with any size that the platform allows.
Previously the input size was limited to 4 Gb, which for 64-bit platform is a totally unnecessary restriction.

Remember, when first talking about the type to use, in the article ["{{ site.TITLE-E1-C}}"]({{ site.ART-E1-C }}),
I mentioned `size_t`. I said then that I
was using `unsigned`, because `size_t` was also unsigned. I didn't, however, use `size_t` itself, because I
considered it an unnecessary complication. Now it turned out that it was actually a performance feature.
This is how writing articles can help you learn something new.

This is what happens when we make everything `size_t`
([see the code in repository]({{ site.REPO-E1-C }}/commit/067109ef4d03529be893ca2fd6205d42edc58c86)):

    $ c++ -O3 -falign-functions=32 -falign-loops=32 -funroll-loops -o e1 e1.cpp -lrt
    $ ./e1
    9Reference: 1367
    11Src_First_1: 987
    11Src_First_2: 986
    11Src_First_3: 1359
    11Dst_First_1: 991
    11Dst_First_2: 766
    11Dst_First_3: 982
    12Dst_First_1a: 763
    12Dst_First_3a: 652
    10Unrolled_1: 636
    12Unrolled_1_2: 632
    12Unrolled_1_4: 636
    12Unrolled_1_8: 635
    13Unrolled_1_16: 635
    15Unrolled_2_Full: 635

The results look really impressive. Let's put them into the table. The old results come from the
[previous article]({{ page.ART-CFAST }}):

<table style="width:100%">
<tr><th>     Version              </th> <th>  Results in Java </th>               <th> Old results in C</th>              <th> New results in C</th></tr>
<tr><td><pre>Reference      </pre></td> <td align="right"><pre>  2860 </pre></td> <td align="right"><pre> 1604</pre></td> <td align="right"><pre> 1367</pre></td></tr>
<tr><td><pre>Src_First_1    </pre></td> <td align="right"><pre>  2481 </pre></td> <td align="right"><pre> 1119</pre></td> <td align="right"><pre>  987</pre></td></tr>
<tr><td><pre>Src_First_2    </pre></td> <td align="right"><pre>  2284 </pre></td> <td align="right"><pre> 1101</pre></td> <td align="right"><pre>  986</pre></td></tr>
<tr><td><pre>Src_First_3    </pre></td> <td align="right"><pre>  4360 </pre></td> <td align="right"><pre> 1458</pre></td> <td align="right"><pre> 1359</pre></td></tr>
<tr><td><pre>Dst_First_1    </pre></td> <td align="right"><pre>  1155 </pre></td> <td align="right"><pre> 1079</pre></td> <td align="right"><pre>  991</pre></td></tr>
<tr><td><pre>Dst_First_1a   </pre></td> <td align="right"><pre>       </pre></td> <td align="right"><pre>  882</pre></td> <td align="right"><pre>  763</pre></td></tr>
<tr><td><pre>Dst_First_2    </pre></td> <td align="right"><pre>  2093 </pre></td> <td align="right"><pre>  880</pre></td> <td align="right"><pre>  766</pre></td></tr>
<tr><td><pre>Dst_First_3    </pre></td> <td align="right"><pre>  1022 </pre></td> <td align="right"><pre> 1009</pre></td> <td align="right"><pre>  982</pre></td></tr>
<tr><td><pre>Dst_First_3a   </pre></td> <td align="right"><pre>       </pre></td> <td align="right"><pre>  727</pre></td> <td align="right"><pre>  652</pre></td></tr>
<tr><td><pre>Unrolled_1     </pre></td> <td align="right"><pre>   659 </pre></td> <td align="right"><pre>  635</pre></td> <td align="right"><pre>  636</pre></td></tr>
<tr><td><pre>Unrolled_1_2   </pre></td> <td align="right"><pre>   654 </pre></td> <td align="right"><pre>  633</pre></td> <td align="right"><pre>  632</pre></td></tr>
<tr><td><pre>Unrolled_1_4   </pre></td> <td align="right"><pre>   636 </pre></td> <td align="right"><pre>  651</pre></td> <td align="right"><pre>  636</pre></td></tr>
<tr><td><pre>Unrolled_1_8   </pre></td> <td align="right"><pre>   637 </pre></td> <td align="right"><pre>  648</pre></td> <td align="right"><pre>  635</pre></td></tr>
<tr><td><pre>Unrolled_1_16  </pre></td> <td align="right"><pre> 25904 </pre></td> <td align="right"><pre>  635</pre></td> <td align="right"><pre>  635</pre></td></tr>
<tr><td><pre>Unrolled_2_Full</pre></td> <td align="right"><pre> 15630 </pre></td> <td align="right"><pre>  635</pre></td> <td align="right"><pre>  635</pre></td></tr>
</table>

Observations:

- All the versions became faster; some became much faster

- All the unrolled versions now run at the same speed; `Dst_First_3a` runs just a bit slower

- Even the `Reference` version became very fast


The code
--------

[Let's have a look at the code generated for `Dst_First_3a`]({{ site.REPO-E1-C }}/commit/067109ef4d03529be893ca2fd6205d42edc58c86):

{% highlight text %}
_ZNK12Dst_First_3a5demuxEPKhmPPh:
        subq    $8, %rsp
        .cfi_def_cfa_offset 16
        cmpq    $2048, %rdx
        jne     .L40
        xorl    %r9d, %r9d
        .p2align 5
.L28:
        movq    (%rcx,%r9,8), %rdi
        leaq    (%rsi,%r9), %rdx
        xorl    %eax, %eax
        .p2align 5
.L29:
        movzbl  (%rdx), %r10d
        movb    %r10b, (%rdi,%rax)
        movzbl  32(%rdx), %r8d
        movb    %r8b, 1(%rdi,%rax)
        movzbl  64(%rdx), %r11d
        movb    %r11b, 2(%rdi,%rax)
        movzbl  96(%rdx), %r10d
        movb    %r10b, 3(%rdi,%rax)
        movzbl  128(%rdx), %r8d
        movb    %r8b, 4(%rdi,%rax)
        movzbl  160(%rdx), %r11d
        movb    %r11b, 5(%rdi,%rax)
        movzbl  192(%rdx), %r10d
        movb    %r10b, 6(%rdi,%rax)
        movzbl  224(%rdx), %r8d
        addq    $256, %rdx
        movb    %r8b, 7(%rdi,%rax)
        addq    $8, %rax
        cmpq    $64, %rax
        jne     .L29
        addq    $1, %r9
        cmpq    $32, %r9
        jne     .L28
        addq    $8, %rsp
        ret
{% endhighlight %}

The code looks much better than before (the previous version was published in the
[previous article]({{ page.ART-CFAST }})). In fact,
it looks close to ideal, To improve the speed even more, something extraordinary must be done.

Retrospective
-------------

In ["{{ site.TITLE-E1-C }}"]({{ site.ART-E1-C }}) we took a **Java** program and converted it into **C++**. Then we've done the following:

- configured function and loop alignment (article: [{{ page.TITLE-UNSTABLE }}]({{ page.ART-UNSTABLE }}));
- optimised memory access to eliminate aliasing effects (article: [{{ page.TITLE-CFAST }}]({{ page.ART-CFAST }}));
- configured loop unrolling (the [same]({{ page.ART-CFAST }}) article)
- replaced `unsigned` with `size_t` (today).

Here is a graph that shows how these steps affected the speeds of different versions:

<img src="{{ site.url }}/images/size-does-matter-graph.png">


Conclusions
-----------
- The size of the data types used does matter.

- The type `size_t` is there for a purpose; this purpose isn't just for programs to look stylish and conform
  to the conventions -- the programs really run faster if they use this type

- We have exhausted conventional ways of achieving high performance in **C**; further improvements require
  some radical change.

- Although most versions now run much faster than their **Java** originals, the peak performance in **C** is the
  same as in **Java**.

- GNU C isn't a bad compiler after all

Coming soon
-----------

Until now all our versions were produced by direct translation form **Java**. Can we do better in some
**C**-specific way? What if we sacrifice portability and write for the specific platform? Can we improve
the speed this way? This will be the topic of my next article.
